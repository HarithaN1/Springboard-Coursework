# Springboard Data Science Mini-Projects

Welcome! This repository contains my data science mini-projects, ranging from data wrangling and statistical inference to machine learning and data visualization.


### Machine Learning

**[Linear Regression with Boston Housing Dataset](https://github.com/andrewjsiu/Springboard-Coursework/blob/master/Machine_Learning_Exercises/linear_regression/Mini_Project_Linear_Regression.ipynb):** I use `scikit-learn` library to build a linear regression model to predict the housing prices in Boston. The various features include per capita crime rate, average number of rooms per dwelling, and pupil-teacher ratio by town. I also split the data into training and testing sets in order to measure how well the model built with the training set can predict the 'unseen' data in the test set. I show how multiple rounds of cross-validation performed on different partitions help limit the problem of overfitting a particular training subset and thus reduce variability of the model.

**[Classification and Logistic Regression](https://github.com/andrewjsiu/Springboard-Coursework/blob/master/Machine_Learning_Exercises/logistic_regression/Mini_Project_Logistic_Regression.ipynb):** I use cross-validation and grid search to find the best regularization parameter **C** for the logistic regression. Regularization applies a penalty for increasing the coefficient estimates in order to reduce overfitting. The regularization parameter **C** in `scikit-learn` is the inverse of the shrinkage parameter lambda. Larger lambda or smaller **C** increases the shrinkage pentalty and shrinks the coefficient estimates toward zero. By default scikit-learn sets **C=1** in logistic regression, so some amount of regularization is used even if **C** is not specified. Regularization is good at reducing the variance of the predictions but increasing the bias at the same time. `GridSearchCV` performs a cross-validated grid-search over a parameter grid. We need to specify an estimation method, parameter values for the estimator and a scoring method. The results show the best estimator, the score of the best estimator, and the parameter setting that yields the best score.

**[Text Classification with Naive Bayes](https://github.com/andrewjsiu/Springboard-Coursework/blob/master/Machine_Learning_Exercises/naive_bayes/Mini_Project_Naive_Bayes.ipynb):** I analyze the movie reviews from the rotten tomatoes database. The goal is to train a classifier to predict whether a critic's movie review is 'fresh' or 'rotten.' To preprocess the text, `CountVectorizer` allows us convert the collection of movie reviews into a matrix of token counts. The parameter `min_df` is used to removed terms that are too rare, and `max_df` is used to remove terms that are too common. I then train a multinomial Naive Bayes classifier assuming that features are conditionally independent given the class. In Naive Bayes, alpha is an additive (Laplace/Lidstone) smoothing parameter. A larger alpha will reduce the variance of the model (and overfitting) but increase bias at the same time. We can think of alpha as a pseudocount of the number of times a word has been seen. In the following code, I use grid search to find the best alpha as well as the best `min_df` that will maximize the probability of observing the training data.

For feature selection, we can create an identity matrix with the size of the number of features/words, each row representing exactly one feature/word. We then use any one word to predict the probabilitiy of freshness or rottenness of a review that contains this word. If one single word can generate high probability of a review being fresh or rotten, that implies this feature has a high predictive power. Reviews containing words such as perfect, touching and masterpiece are likely to be fresh, while words like unfortunately, dull and worst tend to predict rotten reviews.

I also made some improvements of the model in different ways: (1) Include both bigrams and unigrams to capture two-word phrases; (2) Vectorize the reviews based on Term Frequency and Inverse Document Frequency (TF-IDF); and (3) Train a Random Forest classifier with the optimal number of trees chosen by cross-validation.

**[Customer Segmentation using Clustering](https://github.com/andrewjsiu/Springboard-Coursework/blob/master/Machine_Learning_Exercises/clustering/Mini_Project_Clustering.ipynb):** The dataset contains wine offers that were e-mailed to the customers and data on which offers they purchased. Important features of wine offers include wine varietal, the minimum quantity, discount, country of origin and whether or not it is past peak. I merge two spreadsheets and create a pandas dataframe with each row representing a customer and each column representing a wine offer. I first apply K-Means Clustering and use both the elbow method and the Silhouette method to choose the number of clusters. To visualize the clusters, I use Principal Component Analysis to reduce the 32 features into two dimensions. I also compare results from other clustering algorithms: affinity propagation, spectral clustering, agglomerative clustering, and DBSCAN. 

For this dataset, the agglomerative clustering performs well in identifying a group who favor Pinot Noir, a group who tend to choose offers requiring only a minimum quantity of six, and two other groups who tend to purchase large quantities of Champagne (one also purchase Chardonnay and the other Espumante and Prosecco). DBSCAN does not perform well because the sparse dataset of 32 features makes it difficult to find clusters and as a result more than half of the data points are classified as noise. The clustering result from affinity propagation is also not very convincing since some of the clusters tend to overlap in the two-dimensional PCA feature space. Spectral clustering is capable of identifying the Pinot-Noir group, the small-offers group and the Champagne group, but the fourth group does not have a clear pattern. Overall, what is consistent across different clustering methods is that some customers tend to purchase Pinot Noir almost exclusively, some tend to focus only on small offers regardless of the wine varitals, and the rest tend to buy Champagne in bulk.

### Advance Machine Learning Topics


### Data Wrangling

**[JSON Exercises](https://github.com/andrewjsiu/Springboard-Coursework/blob/master/Data_Wrangling/data_wrangling_json/json_exercise.ipynb):** The dataset on the World Bank projects is available in a JSON file. I first load the data as a Pandas dataframe and find that China, Indonesia and Vietnam have the most projects with the World Bank. Then I load the JSON file as a string, normalize the project themes, and find that environment and natural resources management, rural development and human development are the project themes with the highest frequencies. 

**[XML Exercises](https://github.com/andrewjsiu/Springboard-Coursework/blob/master/Data_Wrangling/data_wrangling_xml/xml_exercise.ipynb):** The Mondial database contains geographical terrains and various physical features in the world. I use `xml.etree.ElementTree` module to parse the data, which are stored as elements in a hierarchical structure. Each element has a tag, a number of attributes, a text string, and a number of child elements. I find that (1) Monaco, Japan and Bermuda have the lowest infant morality rate; (2) Shanghai, Istanbul and Mumbai have the largest city population; (3) Han Chinese, Europeans and Indo-Aryan are ethnic groups with the largest overall population, and (4) the longest river in the world is Amazonas, the largest lake is Caspian Sea, and the airport at the highest elevation is El Alto International. 

### Exploratory Data Analysis

**[Human Body Temperature](https://github.com/andrewjsiu/Springboard-Coursework/blob/master/Exploratory_Data_Analysis/data_human_temperature/inferential_statistics_exercise_1_human_temperatures.ipynb):** 
I analyze a dataset of human body temperatures and test whether the true average temperature is 98.6 degrees F. As women have a higher average temperature than men, I also test whether there is such gender difference in temperature.   

